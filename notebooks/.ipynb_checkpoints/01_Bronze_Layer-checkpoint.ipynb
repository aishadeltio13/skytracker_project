{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7636ee06-3963-4652-850c-4a5b05f6da3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîå Iniciando sesi√≥n de Spark...\n",
      "\n",
      "‚úàÔ∏è Procesando VUELOS (Flights)...\n",
      "‚úÖ Vuelos guardados en: s3a://bronze/formatted/flights\n",
      "\n",
      "üé´ Procesando AEROL√çNEAS (Airlines)...\n",
      "‚úÖ Aerol√≠neas guardadas en: s3a://bronze/formatted/airlines\n",
      "\n",
      "üìç Procesando AEROPUERTOS (Airports)...\n",
      "‚úÖ Aeropuertos guardados en: s3a://bronze/formatted/airports\n",
      "\n",
      "üèÜ --- CAPA BRONCE COMPLETADA CON √âXITO ---\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# SCRIPT: 01_Bronze_Layer.py\n",
    "# DESCRIPCI√ìN: Ingesta de CSVs crudos, selecci√≥n de columnas clave\n",
    "# y guardado en formato Parquet (Optimizado) en el Data Lake.\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def create_spark_session():\n",
    "    print(\"üîå Iniciando sesi√≥n de Spark...\")\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"SkyTracker_Bronze_ETL\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "def process_flights(spark):\n",
    "    print(\"\\n‚úàÔ∏è Procesando VUELOS (Flights)...\")\n",
    "    # 1. Lectura\n",
    "    # Ajusta el nombre del archivo si usaste \"flights.csv\" o \"raw_flights.csv\"\n",
    "    df_raw = spark.read.format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(\"s3a://bronze/raw_data/flights.csv\") \n",
    "    \n",
    "    # 2. Transformaci√≥n (Tu selecci√≥n)\n",
    "    df_clean = df_raw.select(\n",
    "        \"YEAR\", \"MONTH\", \"DAY\", \"DAY_OF_WEEK\",\n",
    "        \"AIRLINE\", \"FLIGHT_NUMBER\", \"TAIL_NUMBER\",\n",
    "        \"ORIGIN_AIRPORT\", \"DESTINATION_AIRPORT\",\n",
    "        \"SCHEDULED_DEPARTURE\", \"DEPARTURE_TIME\",\n",
    "        \"DEPARTURE_DELAY\", \"ARRIVAL_DELAY\",\n",
    "        \"DISTANCE\", \"AIR_TIME\",\n",
    "        \"CANCELLED\", \"CANCELLATION_REASON\"\n",
    "    )\n",
    "    \n",
    "    # 3. Escritura (Guardamos como Parquet en la carpeta 'formatted')\n",
    "    # mode(\"overwrite\") borra lo anterior si vuelves a ejecutar el script\n",
    "    ruta_destino = \"s3a://bronze/formatted/flights\"\n",
    "    df_clean.write.mode(\"overwrite\").parquet(ruta_destino)\n",
    "    print(f\"‚úÖ Vuelos guardados en: {ruta_destino}\")\n",
    "\n",
    "def process_airlines(spark):\n",
    "    print(\"\\nüé´ Procesando AEROL√çNEAS (Airlines)...\")\n",
    "    df_raw = spark.read.format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(\"s3a://bronze/raw_data/airlines.csv\")\n",
    "    \n",
    "    # Renombramos para estandarizar\n",
    "    df_clean = df_raw.select(\n",
    "        col(\"IATA_CODE\").alias(\"AIRLINE_ID\"),\n",
    "        col(\"AIRLINE\").alias(\"AIRLINE_NAME\")\n",
    "    )\n",
    "    \n",
    "    ruta_destino = \"s3a://bronze/formatted/airlines\"\n",
    "    df_clean.write.mode(\"overwrite\").parquet(ruta_destino)\n",
    "    print(f\"‚úÖ Aerol√≠neas guardadas en: {ruta_destino}\")\n",
    "\n",
    "def process_airports(spark):\n",
    "    print(\"\\nüìç Procesando AEROPUERTOS (Airports)...\")\n",
    "    df_raw = spark.read.format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(\"s3a://bronze/raw_data/airports.csv\")\n",
    "    \n",
    "    # Eliminamos COUNTRY\n",
    "    df_clean = df_raw.select(\n",
    "        \"IATA_CODE\", \"AIRPORT\", \"CITY\", \"STATE\", \"LATITUDE\", \"LONGITUDE\"\n",
    "    )\n",
    "    \n",
    "    ruta_destino = \"s3a://bronze/formatted/airports\"\n",
    "    df_clean.write.mode(\"overwrite\").parquet(ruta_destino)\n",
    "    print(f\"‚úÖ Aeropuertos guardados en: {ruta_destino}\")\n",
    "\n",
    "# --- EJECUCI√ìN DEL PIPELINE ---\n",
    "if __name__ == \"__main__\":\n",
    "    spark = create_spark_session()\n",
    "    \n",
    "    try:\n",
    "        process_flights(spark)\n",
    "        process_airlines(spark)\n",
    "        process_airports(spark)\n",
    "        print(\"\\nüèÜ --- CAPA BRONCE COMPLETADA CON √âXITO ---\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error cr√≠tico en el pipeline: {e}\")\n",
    "    finally:\n",
    "        spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b300c63c-0885-4c93-bf0a-681ee1908855",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
