**üõ´ Proyecto SkyTracker: Mi implementaci√≥n de un Data Lakehouse End-to-End**

**Autor:** Aisha del Tio de Prado **Tecnolog√≠a:** Docker, PySpark, JupyterLab, MinIO, Power BI

**1. Visi√≥n General del Proyecto**

En este proyecto, he dise√±ado y construido desde cero una soluci√≥n completa de Ingenier√≠a de Datos (Data Lakehouse). Mi objetivo principal ha sido procesar grandes vol√∫menes de datos de tr√°fico a√©reo para transformar archivos crudos en inteligencia de negocio accionable, visualizada en un Dashboard interactivo.

Para ello, he simulado una arquitectura de Big Data real utilizando contenedores y un entorno de desarrollo profesional, **replicando la experiencia de trabajar en plataformas empresariales como Databricks**, asegurando que el proyecto sea modular y escalable.

**2. Mi Stack Tecnol√≥gico**

Para garantizar la portabilidad y la eficiencia del entorno, he seleccionado las siguientes herramientas:

- **Infraestructura:** Desplegu√© todo el ecosistema sobre **Docker** y **Docker Compose**, lo que me permiti√≥ levantar servicios complejos (Spark y MinIO) con un solo comando.
- **Entorno de Desarrollo (IDE):** Utilic√© **JupyterLab** para escribir y ejecutar mi c√≥digo. Esto me permiti√≥ simular el flujo de trabajo de **Databricks**, trabajando con Notebooks interactivos para prototipar, limpiar datos y documentar el proceso paso a paso antes de pasarlo a producci√≥n.
- **Procesamiento:** Utilic√© **Apache Spark (PySpark)** como motor principal por su capacidad de procesamiento distribuido y manejo de grandes vol√∫menes de datos.
- **Almacenamiento:** Implement√© **MinIO** para simular un almacenamiento de objetos tipo S3 (Cloud), creando un Data Lake real.
- **Visualizaci√≥n:** Conect√© **Microsoft Power BI** para la capa de presentaci√≥n final.

**3. Arquitectura de Datos: Mi Enfoque "Medallion"**

Estructur√© el flujo de datos en tres capas l√≥gicas para refinar la informaci√≥n progresivamente:

**ü•â Capa Bronze (Ingesta y Formato)**

Mi primer paso fue la ingesta de los datos crudos (flights.csv, airlines.csv, airports.csv).

- **Decisi√≥n t√©cnica:** Decid√≠ no trabajar con los CSVs directamente en las siguientes fases. En su lugar, convert√≠ todo a formato **Parquet**.
- **¬øPor qu√©?** Para optimizar el rendimiento de lectura en Spark y reducir el espacio en disco gracias a la compresi√≥n columnar.

**ü•à Capa Silver (Limpieza y Enriquecimiento)**

Aqu√≠ es donde realic√© la mayor carga de ingenier√≠a. Mi objetivo fue crear una **Tabla Maestra** unificada.

- **El Reto de los Joins:** Me encontr√© con la necesidad de cruzar la tabla de vuelos con la de aeropuertos *dos veces* (una para el origen y otra para el destino).
- **Mi Soluci√≥n:** Utilic√© **Alias** en Spark (df.alias("origen") y df.alias("destino")). Esto me permiti√≥ enriquecer cada vuelo con las coordenadas geogr√°ficas de salida y llegada sin generar conflictos de nombres en las columnas.
- **Gesti√≥n de Calidad del Dato (Data Quality):** Detect√© que el dataset conten√≠a c√≥digos de aeropuerto num√©ricos (ej. 10135) que no cumpl√≠an el est√°ndar IATA y generaban valores nulos. Implement√© un filtro estricto (length == 3) antes de los cruces para garantizar la integridad de la Tabla Maestra.
- **Feature Engineering:** Cre√© nuevas variables de negocio, como transformar la hora num√©rica en franjas horarias (DEPARTURE\_HOUR) y los d√≠as de la semana en nombres legibles (DAY\_NAME), aplicando la filosof√≠a *"Write Once, Read Many"* (procesar una vez en Silver para facilitar m√∫ltiples lecturas en Gold).

**ü•á Capa Gold (Agregaciones de Negocio)**

En esta etapa final, dej√© de lado el detalle vuelo a vuelo para centrarme en responder preguntas de negocio mediante agregaciones (KPIs). Gener√© tres tablas espec√≠ficas:

- **Heatmap de Fiabilidad:** Calcul√© el retraso medio por d√≠a y hora. Afin√© la l√≥gica para que los adelantos (tiempo negativo) contaran como 0, evitando que "maquillaran" los datos de retraso real.
- **Estr√©s de Rutas:** Agrup√© los datos por par Origen-Destino, conservando las coordenadas geogr√°ficas para poder proyectar el tr√°fico en mapas.
- **Eficiencia de Pilotos:** Filtr√© solo los vuelos con retraso inicial para medir qu√© aerol√≠neas son capaces de recuperar tiempo en el aire (Departure - Arrival), identificando la eficiencia operativa real.

**4. Retos Superados y Soluciones**

Durante el desarrollo, me enfrent√© a varios obst√°culos t√©cnicos que resolv√≠ de la siguiente manera:

- **Integraci√≥n Power BI - MinIO:** Power BI no pod√≠a interpretar los metadatos internos de MinIO (xl.meta). Para solucionarlo, desarroll√© un script de exportaci√≥n en Spark que vuelca los datos de la capa Gold desde el sistema distribuido a una carpeta local mapeada, generando archivos Parquet est√°ndar legibles por Windows.
- **Lectura de Particiones:** Spark divide los datos en m√∫ltiples archivos (part-00000...). Aprend√≠ a utilizar el conector de "Carpeta" en Power BI, aplicando filtros de extensi√≥n para ignorar archivos de control como \_SUCCESS y combinando los binarios correctamente.
- **Manejo de Nulos en Visualizaci√≥n:** Al principio, mi mapa mostraba valores nulos. Tras auditar los datos en mi **Notebook de pruebas (Sandbox) en JupyterLab**, identifiqu√© la inconsistencia de los c√≥digos num√©ricos y apliqu√© la limpieza en la capa Silver, logrando una visualizaci√≥n geogr√°fica completa.

**5. Conclusi√≥n**

Este proyecto me ha permitido consolidar mis conocimientos en el ciclo de vida completo del dato. He pasado de tener archivos dispersos a construir un **Data Pipeline robusto, automatizado e idempotente**, capaz de alimentar un cuadro de mando profesional que permite tomar decisiones basadas en datos reales, utilizando herramientas est√°ndar de la industria.

![Resumen Arquitectura](flujo-trabajo.png)

![Resultado PoweBi](power-bi.png)

![JupyterLab](im1.png)

![MinIO Console](im2.png)

